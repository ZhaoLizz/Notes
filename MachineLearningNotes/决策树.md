# 决策树
## 1.基本流程
* 概述:决策树是基于树结构来进行决策的.决策过程的最终结论对应了我们所希望的判定结果,决策过程中提出的每个判定问题都是对某个属性的测试,其考虑范围是在上次决策结果的限定范围  
* 目的是产生一棵**泛化能力强,即处理未见示例能力强的决策树**
* 生成决策树的递归过程返回的情况:
    1. 当前结点包含的样本全属于同一类别.
    2. 当前结点的属性集为空,或是所有样本在所有属性上的取值相同,无法划分: 将当前结点标记为叶节点,并将其类别设定**为该结点所含样本最多的类别**
    3. 把当前结点标记为叶节点,将其类别设定为**其父节点所含样本最多的类别**

## 2.划分选择
随着划分过程不断进行,我们希望决策树分支结点所包含的样本尽可能**属于同一类别**,即结点的**纯度**越来越高
#### 2.1信息增益
* **信息熵的定义**是度量样本集合纯度最常用的一种指标(描述数据的无序度).假定`当前样本集合D`中`第k类样本所占的比例`为 $p_k$,(k = 1,2...n).*样本集D最多划分为n类*则D的信息熵定义为:  
$Ent(D) =- \sum_{k=1} ^ n p_k\log_2p_k$ 
    * Ent(D)的值越小,D的纯度越高
* **信息增益的定义**:
    * 假定离散属性a有V个可能的取值{a_1 ... a_v},若**使用a对样本集D进行划分**,则会产生V个分支结点,**其中第v个分支节点包含了D中所有在属性a上取值为$a_v$的样本**,记为$D^v$.
    * 考虑到不同的分支节点所包含的样本数不同,给分支节点赋予**权重**:`|$D^v$|/|D|`,即样本越多的分支节点的影响越大.
    * 信息增益:  
$Gain(D,a) = Ent(D) - \sum_{v = 1}^ V\frac{|D^v|}{  |D|} Ent(D^v)$    
用父节点的信息熵减去每个子节点的权重乘信息增益
    * 信息增益越大,则意味着使用`属性a`来对样本集D进行划分所获得的**纯度提升**越大(熵的减少或数据无序度的减少).因此,可以用信息增益来进行决策树的划分属性选择
#### 2.2增益率
* **信息增益准则对可取值数目较多的属性有所偏好**,为减少这种偏好可能带来的不利影响(*比如用"编号"作为划分属性,对数据集大小为n的D将产生n个划分,每个分支的纯度已达到最大,这样的决策树不具有泛化能力,无法对新样本进行预测*),使用**增益率**来选择最优划分属性.  
* 增益率定义为:  
$GainRatio(D,a) = \frac{Gain(D,a)}{IV(a)}$  
其中属性a的`固有值IV(a)`:  
$IV(a) = -\sum_{v = 1} ^ V \frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}$
    * 属性a的可能取值数目越多,IV(a)的值越大
    * **增益率准则对可取值数目较少的属性有所偏好**

#### 2.3基尼指数
* 基尼值:描述数据集D的纯度  
$Gini(D) = \sum_{k = 1} ^ n \sum _{k^{'} \neq k} p_kp_{k^{'}}$  
$= 1 -  \sum_{k = 1} ^ n p_k^2$
    * `Gini(D)`反映了**从数据集D中随机抽取两个样本,其类别标记为不一致的概率**.因此,Gini(D)越小,数据集D的纯度越高
* 基尼指数:
$GiniIndex(D,a) = \sum^V_{v = 1} \frac{|D^v|}{|D|}Gini(D^v)$
    * 于是可以在**候选属性集合**A中,选择那个使得划分后基尼指数最小的属性作为最优划分属性

## 3.剪枝处理
剪枝处理是为了避免**过拟合**.

#### 3.1预剪枝
* 策略:在决策树生成过程中,对每个结点在划分前进行估计,若**当前节点的划分不能带来决策树泛化性能的提升**,则停止划分并将当前节点标记为叶节点.

#### 3.2后剪枝
* 策略:先从训练集生成一棵完整的决策树,然后**自底向上**地对**非叶节点**进行考察,**若将该结点对应的子树替换为叶节点能带来决策树泛化性能提升**,则将该子树替换为叶节点
* 后剪枝策略通常比预剪枝策略保留了更多的分支.欠拟合风险很小,泛化性能往往优于预剪枝决策树.但后剪枝过程是在生成完全决策树之后进行的,并且要自底向上对树中所有非叶节点进行逐一考察,因此其训练时间开销大

## 4.连续与缺失值

#### 4.1连续值处理
采用二分法对连续属性进行处理:  
* 给定样本集D和连续型属性a,假定a在D上出现了n个不同的取值,将这些值从小到大排序,记为{$a^1...a^n$}.基于`划分点t`可将D分为子集$D_t^-$(*在属性a上取值不大于t的样本*)和$D_t^+$(*在属性a上取值大于t的样本*).**对相邻的属性取值$a^i,a^{i+1}$来说,t在区间[$a^i,a^{i+1}$)中取任意值所产生的划分结果相同.**
* 因此,对连续型属性a,我们可考察包含n-1个元素的候选划分点集合  
$T_a = \lbrace \frac{a^i + a^{i + 1}}{2} | 1 \leq i \leq n-1 \rbrace$.
    * 即:把区间[$a^i,a^{i+1}$)的中位点$\frac{a^i + a^{i + 1}}{2}$作为候选划分点.然后就可以像离散属性值一样来考察这些划分点

#### 4.2缺失值处理
章节未读

## 4.5 多变量决策树
在多变量决策树中,非叶节点不再是仅对某个属性,而是**对属性的线性组合进行测试**.每个非叶节点是一个形如$\sum^d_{i = 1}w_ia_i = t$的线性分类器.其中$w_i$是属性$a_i$的权重,该权重可在**该结点所含的样本集和属性集**上学得.
