# 神经网络基础

## Logistic Regression

### Cost Function  

* Loss fucntion  
The loss fucntion measures the discrepancy between the pridiction($\hat y ^ i$) and the desired output ($y^i$).The loss function computers the error for a **single training example**  
$L(\hat y ^ i,y^i) = -(y^i log(\hat y^i) + (1 - y^i) log(1 - \hat y^i))$

* Cost function  
The  cost fucntion is the average of the loss fucntion of the **entire training set**.We are going to find the parameters w and b that minizmize the overall cost function.   
 ![](https://ws1.sinaimg.cn/large/006QRhAtgy1fplwsx4ho6j30kr02pt8q.jpg)


# 浅层神经网络

## 一.计算一个神经网络的输出
* 注意这里w应该是(3,3),b是(3,1)
![](http://www.ai-start.com/dl2017/images/L1_week3_5.png)

#### Note:
* x表示输入特征,a表示每个神经元的输出,W表示特征的权重,**上标表示神经网络的层数**,**下标表示该层的第几个神经元**
#### 神经网络的计算
* 计算单个神经元  
![](http://www.ai-start.com/dl2017/images/L1_week3_6.png)  
对隐藏层的神经元计算,最终得到隐藏层各个神经元作为下一层的输出节点的结果:  
![](https://ws1.sinaimg.cn/large/006QRhAtly1fpo63z1t11j307x04tmx5.jpg)

#### 向量化计算

向量化的过程就是**将神经网络中的一层神经元参数纵向堆积起来**  
例如隐藏层中w纵向堆积起来

![](https://ws1.sinaimg.cn/large/006QRhAtly1fpo68faifbj306304et8j.jpg)  
![](https://ws1.sinaimg.cn/large/006QRhAtly1fpo6ibdxprj30b2059gln.jpg)

## 二.多样本向量化
逻辑回归是将各个训练样本组合成矩阵,对矩阵的各列进行计算.神经网络时通过对逻辑回归中的等式简单的变形,让神经网络计算出输出值.这种计算用向量化是所有的训练样本同时进行的.

![](http://www.ai-start.com/dl2017/images/L1_week3_8.png)  
* 用第一个训练样本$x^{[1]}$计算出预测值$\hat y^{[1]}$,这就是第一个训练样本上计算得出的结果,然后依次用$x^{[m]}$计算出预测值$\hat y^{[m]}$.因此可以用向量化来表示:  
![](https://ws1.sinaimg.cn/large/006QRhAtly1fpo6zhwrr5j30ax0cd0st.jpg)  
* 参数之间的关系:  
![](https://ws1.sinaimg.cn/large/006QRhAtly1fpo70n113nj30dn03u74c.jpg)


## 三.激活函数
![](http://www.ai-start.com/dl2017/images/L1_week3_9.jpg)

* sigmoid激活函数: $a = \sigma(z) = \frac 1 {1 + e ^ {-z}}$
* tanh激活函数: $a = tanh(z) = \frac {e^z - e^{-z}} {e^z + e^{-z}}$ 
    * tanh函数是sigmoid的向下平移和伸缩后的结果.
    * 如果在**隐藏层**上使用tanh,效果总是犹豫sigmoid函数.因为函数值域在-1 和+1之间的激活函数的均值更接近0.在训练模型时,如果使用tanh代替sigmoid来中心化数据,会使得数据的平均值更接近0而不是0.5
    * 例外情况是,在**二分类问题**中,对于**输出层**,因为y的取值是0或1,所以想让$\hat y$的值介于0和1之间,而不是+-1.这时可以**在隐藏层使用tanh激活函数,仅仅输出层使用sigmoid函数**
* tanh和sigmoid的共同缺点是**在z特别搭或者特别小的情况下,导数的梯度会或者函数的斜率会变得特别小,最后就会接近0,导致降低梯度下降的速度**
* 当今最常用的激活函数:**修正线性单元函数ReLu**: $a = max(0,z)$.
    * ReLu函数在z=0的时候导数没有定义,然而在实际工程中,z的值刚好等于0.0000000的出现概率相当小,无需担心.当z真的出现0的时候,假设导数是1或者0都可以
    * Relu在z的区间变动很大的情况下,激活函数的导数或者激活函数的斜率都会远大于0,学习的更快
* Leaky Relu: $a = max(0.01,z)$,这里可以为学习算法选择不同的参数

#### 选择激活函数的经验
* 如果是输出0,1值的二分类问题,则输出层选择sigmoid函数,其他的所有单元都选择Relu函数

## 四.选择非线性激活函数的意义
在神经网络正向传播过程中,如果去掉激活函数或者说令线性激活函数$g(z) = z$,则$a^{[1]}$ = $z^{[1]}$,则  
* $a^{[1]} = z^{[1]} = w^{[1]}x + b^{[1]}$
* $a^{[2]} = z^{[2]} = w^{[2]}a^{[1]} + b^{[2]}$ =  
  $w^{[2]}(w^{[1]}x + b^{[1]}) + b^{[2]}$ =  
  $w'x + b'$

由此得出,**这里线性隐层没有任何作用**,因为这两个线性函数的组合本身就是线性函数,除非引入非线性函数,否则输入和输出仍然是线性关系,这个模型的复杂度和没有任何隐藏层的标准Logistic回归是一样的.

## 五.激活函数的导数
* sigmoid激活函数
![](http://www.ai-start.com/dl2017/images/L1_week3_10.png)  
$\frac d {dz} g(z) = \frac 1 {1 + e^{-z}} (1 - \frac 1 {1 + e^{-z}}) = g(z) (1 - g(z))$

* tanh激活函数  
![](http://www.ai-start.com/dl2017/images/L1_week3_11.png)   

$g(z)  = tanh(z) = \frac {e^z -e^{-z}}{e ^z + z ^{-z}}$   
$\frac d {dz}g(z) = 1 - (tanh(z))^2$
* Relu激活函数  
![](http://www.ai-start.com/dl2017/images/L1_week3_12.png)  
$g(z) = max(0,z)$

![](https://ws1.sinaimg.cn/large/006QRhAtly1fpr445ix5ij307z02sglg.jpg)

* LeakyLinearUnit  
![](https://ws1.sinaimg.cn/large/006QRhAtly1fpr45bss75j308l03ut8m.jpg)

## 随机初始化
**参数b可以初始化为0,权重w不能初始化为0**:  
* 如果把w初始化为0,那么同一层的神经元$a_1^{[1]}$,$a_2^{[1]}$就会相等,在做反向传播计算时,导致$dz_1^{[1]}$,$dz_2^{[1]}$也相等,优化得到的权值也相等.最终经过每次训练的迭代,这两个隐藏神经元仍然是同一个函数,所有的隐藏单元都是相同的,没有任何意义
* 解决方法就是**随机初始化参数**.把$W^{[1]}$设为`np.random.randn(2,2)`(根据高斯分布生成2*2的矩阵).**通常还需要乘一个小的常数**,比如0.01,这样把它初始化为很小的随机数.**这个随机数是为了避免w初始值太大**,导致sigmoid函数或者tanh函数停留在相对平坦的区域,使得梯度下降进行的很慢
* 可以把b初始化为0,因为只需随机初始化W,就会有不同的隐藏神经元计算不同的数值,因此b为0时不会有类似w的对称问题
![](http://www.ai-start.com/dl2017/images/L1_week3_13.png)  

![](https://ws1.sinaimg.cn/large/006QRhAtly1fprhxihrmfj30ek023aa1.jpg)


## 核对矩阵的维数
一个有效debug的重要方法:**检查算法中矩阵的维数**  
* $n^{[l]}$:第$l$层神经元的个数
* $dw^{[l]} = w^{[l]} : (n^{[l]},n^{[l-1]})$
* $db^{[l]} = b^{[l]}: (n^{[l]},1)$,列向量
* $z^{[l]} = a^{[l]} : (n^{[l]},1)$,列向量


w,b向量化后维度都不改变,但z,a.x的维度会改变,m是训练集大小,向量化后:
* $Z^{[l]}$(针对整个训练集)是由每一个单独的$Z^{[l]}$叠加而成,即m列列向量叠加, $Z^{[l]} : (n^{[l]},m)$
* $A^{[l]}:(n^{[l]},m)$