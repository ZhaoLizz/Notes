# 超参数调试

## 1. 调试处理

* 超参数的重要程度:

  ​	 $\alpha > \beta(momentum) = hidden units = minibatchsize  > layers =learning rate decay $

* 超参数调试方法:网格搜索法

  * 在参数的数量很少的时候,可以遍历所有的超参数组合,选出参数效果最好的那一组
  * 更快的方法是 **随机选择点**:随机选择几组取值.超参数1和超参数2的取值数量可以不同,可以任意组合.这种方法会更为高效.

  ![](http://ww1.sinaimg.cn/large/0077h8xtly1ftqogd8o1wj30ne09x76q.jpg)

* 采用 **由粗糙到精细的策略**

  如果我们发现某个区域的取值效果都很好,接下来就要**放大这块小区域,然后在其中更密集地随机取值**.

  ![](http://ww1.sinaimg.cn/large/0077h8xtly1ftqokh6fvgj30fv09zn02.jpg)



## 2. 为超参数选择合适的范围

在超参数范围中随机取值可以提升搜索效率.但是随机取值需要在 **有效范围内随机均匀取值**,需要选择合适的标尺.对于有的参数,比如层数,就可以线性取值,如1,2,3,4.但是对于某些超参数而言是不适用的.

* 比如$alpha$,取值从0.0001到1,如果从0.0001到1随机均匀取值,那么90%的数值会落在0.1到1之间. 因此,使用 **对数标尺**搜索超参数范围的方式更合理: 这里不使用线性轴,取值为0.0001,0.001,0.01,0.1,1.依次为10^-4^,10^-3^,10^-2^等.

```python
r = -4 * np.random.rand() # [-4,0]的随机列表
a = 10**r
```

![](http://ww1.sinaimg.cn/large/0077h8xtly1ftqq4xlke3j30mp047ta9.jpg)

* 另一种常见的例子是给指数加权平均的$\beta$取值.假设我们认为beta取值从0.9到0.999.如果使用线性轴,取值就是0.91,0.92,0.93...,效果非常不好.因此我们考虑 **1 - beta (取值范围为[0.1,0.001])**,然后用对数标尺搜索.

## 3. 归一化深度隐藏层:Batch Norm

目标是归一化深度网络的**隐藏层**,比如z^[1]^,z^[2]^.  **归一化操作不仅作用于输入层,也同样视同于神经网络的深度隐藏层**

$\mu = \frac{1}{m} \sum z^{i} $

$\sigma^2 = \frac{1}{m}\sum {(z^i - \mu)^2}$

$z^{(i)}_{norm} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}$    

* 这里$epsilon$的作用是使得数值稳定,防止$\sigma$的值为0

现在某一层的z值已经标准化(均值0,标准差1),但是 **我们并不想让隐藏层总是函数均值0和方差1**,因为隐藏层或许不同的分布会有不同的意义.因此我们计算一个 **加权的归一化数值  **  

$\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta$

* 这里$\gamma,beta$都是模型需要学习的参数,可以使用梯度下降或其他算法来更新这两个数值,就和更新权重一样.

## 4. 在神经网络中使用Batch Norm

### 算法实现

![](http://ww1.sinaimg.cn/large/0077h8xtly1ftr0ohl1krj30qz09sq9q.jpg)



1. 首先由上一层的输入计算出z^[L]^的值.
2. 通过z^[L]^的每一个分量对z^[L]^进行 **归一化**,然后**加权**$\gamma z + \beta$得到$\tilde{z}^{[L]}$
   * 这里的z^[L]^是(m,1),因此$\gamma,\beta$也是(m,1)
3. 通过 **激活函数**计算出a^[L]^
4. 继续下一层的计算

在使用BP算法时,计算每一层的$dw^{l},d\gamma^{l},d\beta^{l}$ ,然后更新所有的这些参数

![](http://ww1.sinaimg.cn/large/0077h8xtly1ftr1j1zbs3j30qv06nq6v.jpg)

### 计算中的一个关于b的细节问题:

先看z的计算方式: z^[L]^=w^[L]^a^[L-1]^ + b^[L]^.由于归一化结果为 **均值0,方差1**,在此过程中,每项加的常数都会抵消掉,**常数项不会影响归一化的结果**.因此**可以在网络中消除参数b**,或者把b置为0.

最终的结果就是z^[L]^=w^[L]^a^[L-1]^

![](http://ww1.sinaimg.cn/large/0077h8xtly1ftr1j1zbs3j30qv06nq6v.jpg)

## 5. Batch Norm奏效的原因

### Covariate shift问题

* Covariate shift: 如果已经学习了x到y的映射,这时如果**x的分布**改变了,那么就需要重新训练学习的算法

  在神经网络中,如果前面某一层的参数改变了,则传递到当前层的输入值也会相应地改变,因此就有了Covariate shift的问题.

  而Batch归一化做的就是**减少这些隐藏值分布的变化**.当神经网络在之前层中更新参数的时候,Batch归一化可以确保每一层的参数无论怎么变化,**至少输入值的均值和方差会一直为0和1**(也有可能不是0和1,但是一个确定的值,由当前层的gamma和beta决定).因此Batch Norm **限制了前层参数更新对数值分布影响的程度**,使得输入值的分布更加稳定,因此每一层网络都要自己学习,稍稍独立于其他层.

  ![](http://ww1.sinaimg.cn/large/0077h8xtly1fts3y0mfb3j30q90cztim.jpg)

  ### Batch Norm带来了轻微的正则化

![](http://ww1.sinaimg.cn/large/0077h8xtly1fts40h3y4hj30qj0amn3v.jpg)

* dropout是在每个隐藏层的输入值上以一定的概率增加了噪声0或1.Batch Norm类似于dropout,归一化也带来了额外的噪声,使得后层的单元不过分依赖于任何一个前层的单元.
* 如果mini-batch很大的话(比如512),就会一定程度上减小噪声,也就减小了正则化的效果
* 注意Batch Norm仅仅能带来 **轻微的**正则化效果,因此只能把它当做 **归一化隐藏单元的输入值并加速学习的方式**



## 6. Soft max回归

用于多分类任务.即 **在网络的最后一层使用一个softmax激活函数,把最后一层的输入矩阵转换为对应每一分类的概率**

![](http://ww1.sinaimg.cn/large/0077h8xtly1fts68js638j30qv07r0ww.jpg)

### 实现步骤

* $z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}$

* Activation function:

  * $t = e^{z^{[l]}}$
  * $a^{[l]} = \frac{e^{z^{[l]}}}{\sum_{j =1 }^m t_j} $,此时a向量即是对应的分类概率

  

 ![](http://ww1.sinaimg.cn/large/0077h8xtly1fts6fst0xfj30qp08fwjn.jpg)

![](http://ww1.sinaimg.cn/large/0077h8xtly1fts6ggjq6ej30si0dxk0h.jpg) 

###  Softmax的损失函数



