此L2范数正则化也被称为"权重衰减"## 一.训练集,验证集,测试集
* 在数据量很小时,70%训练集,30%测试集或者60%训练集,20%验证集和20%测试集
* 对于百万级别的数据量,验证集和测试集占总数据量的比例会趋向于更小.

## 二.训练神经网络的基本方法
![](http://www.ai-start.com/dl2017/images/L2_week1_8.png)  
* 首先检查是不是高变差,如果偏差较高以至于无法拟合训练集,就需要选择一个新的网络,比如含有更多隐藏层或隐藏单元的网络,或者花费更多时间来训练网络,或者使用更先进的优化算法,直到可以拟合数据为止
* 如果偏差过高,**最好的办法是采用更多的数据**.如果没有更多数据,考虑使用正则化来减少过拟合,或者使用更合适的神经网络框架(有可能合适的神经网络会使得偏差和方差同时降低)

## 三.正则化

### 逻辑回归的正则化
![](http://www.ai-start.com/dl2017/images/fa185e95684bbe6c0e9100164aff2ee5.png)  
* w的欧几里得范数平方: $||w||^2 = \sum_j^nw_j^2 = w^Tw$

为什么只对w参数正则化处理而不加上b?因为**w通常是一个高维的参数矢量**,包含很多的参数,可以表达高偏差的问题.而b仅仅是一个数字,只是众多参数中的一个,对高方差的影响作用很小

## 四.神经网络的正则化
![](https://ws1.sinaimg.cn/large/006QRhAtly1fpsvwx2il2j30id064juy.jpg)  
* 神经网络的成本函数包含从$W^{[1]},b^{[1]}$到$W^{[l]},b^{[l]}$的所有参数,$l$是神经网络的层数,因此损失函数为m个训练样本损失函数的总和乘1/m.
* 正则项为$\frac {\lambda} {2m} \sum _1^L||W^{[l]}|| ^2$,费罗贝尼乌斯范数定义为**矩阵中所有元素的平方求和**,用下标F表示.  
    * $\sum _1^L||W^{[l]}|| ^2 = \sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$ : 第$l$层的w矩阵所有元素的平方求和

## 五.利用正则化的梯度下降
用backprop计算出dW的值,**然后在dW的基础上增加一个正则化项**.即:  
* $dw^{[l]} = (from backprop) + \frac \lambda m w^{[l]}$
* $w^{[l]} := w^{[l]} - \alpha dw^{[l]}$,即  
    $w^{[l]} := w^{[l]} - \alpha [(frombackprop) + \frac \lambda mw^{[l]}]$

该正则项说明,不论$W^{[l]}$是什么,我们都试图让它变的更小.给矩阵W乘以$(1-\alpha \frac \lambda m)$倍的权重,即相当于W减去$\alpha \frac \lambda m$倍的它.由于系数$(1-\alpha \frac \lambda m)$小于1,因此L2范数正则化也被称为"权重衰减"

## 六.Dropout随机失活正则化
对每一层神经网络的每个神经元进行随机失活使整个网络变小

### 1.实施dropout的方法:反向随机失活
1. 对于一个$l=3$的三层神经网络,首先定义一个dropout向量$d^{[3]}$表示一个三层的dropout向量:  
`d3 = np.random.rand(a3.shape[0],a3.shape[1])`  
    * d3表示$d^{[3]}$,同样a3表示$a^{[3]}$,是神经网络中第3层的向量
    * d3的行列和a3的行列都对应一致
2. 然后根据**阈值keep-prob**生成随机向量d,keep-porb是一个具体的数字,表示**保留某个隐藏单元的概率**,它的作用是生成随机矩阵.
    * 例如keep-prob=0.8,则$d^{[3]}$矩阵中值为1的概率是0.8,值为0的概率是0.2
    * `d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keep-prob` 
3. 接下来对$a^{[3]}$进行随机失活处理,即$a^{[3]} := d^{[3]} * a^{[3]}$,(叉乘:对应位置元素相乘),让$a^{[3]}$中相对应的元素归零
    * `a3 = np.multiply(a3,d3)`
4.  最后令$a^{[3]}$除以keep-prob参数
    * `a3 /= keep-prob`
    * 令$a^{[3]}$除以keep-prob参数的解释:  
    $z^{[4]} = w^{[4]}a^{[3]} + b^{[4]}$,由于$a^{[3]}$中由20%的元素被归零,为了不影响$z^{[4]}$的期望值,需要用$w^{[4]}a^{[3]}/0.8$,这样就会保证$a^{[3]}$的期望值不会变
### 2.在测试阶段不要使用dropout函数
在测试阶段,我们不希望输出随机结果.

### 3.关于dropout的相关解释
直观上理解:神经元不能依赖于任何一个特征,因为该单元的某个输入可能被清除.  
对于某些容易发生过拟合的层,可以把keep-porb的值设置得比其它层更低.

## 七.其他正则化方法
### 1. 数据扩增
比如图片做为训练集,为了获得更多的训练数据,可以把图片水平翻转或者裁剪拉伸.对于数字识别,可以把数字扭曲.虽然这些数据没有全新的数据包含的信息多,但是这么做几乎没有花费.  

### 2. early stopping
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq5hd55f7cj30n709a409.jpg)

训练误差和方差会有上图这种趋势.在迭代次数比较少的时候,参数w接近0.因为随机初始化w的时候它们的值都是比较小的随机值.在迭代和训练过程中w越来越大,所以**early stopping要做的就是在中间点停止迭代过程**,得到一个中等大小的w.

## 八.归一化输入值
### 1. 均值归一化
* 样本均值 : $\mu = \frac 1 m \sum _{i = 1}^m x$  
* 归一化 : $x := x - \mu$  

![](https://ws1.sinaimg.cn/large/0077h8xtly1fq5ix5mij8j30e904yjs4.jpg)

### 2. 方差归一化
* 样本方差: $\sigma ^2= \frac 1 m \sum _{i = 1}^m(x^{(i)})^2$
    * 由于已经进行了样本均值归一化,所以计算样本方差时不用每个样本减去样本均值
    * $\sigma ^ 2$是一个向量,它的每个特征都有方差.
* 方差归一化: $x := x / \sigma ^ 2$

![](https://ws1.sinaimg.cn/large/0077h8xtly1fq5jbvh4pnj30e704xmxy.jpg)

## 九. 避免梯度爆炸/消失的有效措施:权重初始化
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq5lmioi70j309h06paal.jpg)  
1. 对于一个神经元来说,$z = w_1x_1 + w_2 x_2 + ... + w_nx_n + b$  
这里暂时忽略b,b = 0.  
2. 为了预防z值过大或过小,我们希望n越大,$w_i$越小.因为z是$w_ix_i$各项的和,如果把很多此类项想家,我们希望每项的值变小.  
3. 因此合理的方式是设置$w_i = \frac 1 n$
    * n表示神经元的输入特征的数量.即每个**x**向量的特征数
    * `w = np.random.randn(shape) * np.sqrt(1/n_(l-1))`
        * `n_(l-1)`就是第l层神经元输入特征的数量,即l-1层神经元的数量

* 对于Relu激活函数,最好把方差设置为`2/n`
* 对于tanh激活函数,最好设置为`1/n`

由于给权重w设置了合理的值,它不能比1大或小太多,所以没有梯度爆炸或者消失过快


## 十. 一个有效的debug方法:grad check梯度检验
假设网络中有如下参数: $w^{[1]},b^{[1]} .....w^{[l]},b^{[l]}$.

1. 首先把所有的参数转换为向量,这里利用代价函数$J(\theta)$,即:  
    $J(w^{[1]},b^{[1]} ...w^{[l]},b^{[l]}) = J(\theta)$
2. 同样把所有dW,db转换为向量,然后对每个$\theta$计算**偏导逼近值**

    for each i :   
    ![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6721u7hlj30fp01vglh.jpg)

    * $d\theta _{approx} [i] \approx \frac {\partial J}{\partial \theta_i} = d\theta_i$
3. 下面检查每项 $d\theta _{approx} [i]$ 和$\frac {\partial J}{\partial \theta_i}$之间的约等于的程度:  
    首先把$d\theta _{approx} [i]$和$\frac {\partial J}{\partial \theta_i}$转换为矩阵,然后利用**欧几里得范数**计算:  

    $\frac {||d\theta_{approx} - d\theta||_2}{||d\theta_{approx}||_2 + ||d\theta||_2}$

    ![](https://ws1.sinaimg.cn/large/0077h8xtly1fq686dlz1nj30kc06etay.jpg)
    * 计算这两个向量的距离,**角标2表示先计算平方和,然后求平方根**,得到欧式距离
    * 分母表示向量长度归一化,避免这些向量过大或过小
4. 检查上式的值的范围(这里假设$\varepsilon = 10 ^{-7}$ ):
    * 如果值 < $\varepsilon$,则说明导数逼近很有可能是正确的
    * 如果值在$10^{-5}$范围内,则这个值有可能是对的,有可能不对
    * 如果值$> 10 ^{-3}$则很有可能出现了bug,应该仔细检查所有的$\theta$项,试图找一出导致这个数变大的具体的i值,用它来追踪一些求导计算是否正确.

### 梯度检验的注意事项
#### 1. 不要在训练中使用梯度检验,它仅用于调试
计算所有i值的偏导逼近值$d\theta_{approx}$是一个很漫长的计算过程.因此只有在需要调试的时候才使用它,调试完毕后关闭梯度检验.
#### 2. 如果梯度检验没有通过,要检查所有项
如果存在一个$d\theta_{approx}[i]$与$d\theta[i]$相差太多,我们就要找出这个`i`值.  
* 例如,某些层之间的$\theta$或者$d\theta$相差很大,但是$dw^{[l]}$的各项非常接近,这时有可能在计算$db$的过程中存在bug.反过来b或db很接近,有可能计算dw有bug

#### 3. 在实施梯度检验时,如果使用正则化,损失函数要记得加上正则项
$J(\theta) = \frac 1 m \sum(\hat y^{(i)},y^{(i)}) + \frac \lambda {2m} \sum ||W^{[l]}||^2$

#### 4. 梯度检验不能与dropout同时使用
在每次迭代过程中,dropout会随机消除隐藏层的某些神经元,导致难以计算dropout在梯度下降上的代价函数J.  