# 优化算法

## 一.Mini-batch梯度下降

### 1. Mini-batch梯度下降算法
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6h8njhupj30m108r444.jpg)
* **把一个很大的数据集切分为很多相同大小的子数据集**,把这些子数据集组成矩阵然后计算可以提高计算效率.
* 用大括号来表示第几个mini子数据集

在进行梯度下降算法时,每次同时处理单个mini子数据集$X^{\{t\}},Y^{\{t\}}$

```
# 最外层循环针对epoch反复训练,即遍历了整个mini子数据集
repeat{
    # 假设把数据集分成5000个mini子数据集
    for t in 5000{      
        Forword prop on X^{t}:  # 前向传播计算出网络输出值
            Z^[1] = w^[1]X^{t} + b^[1]
            A^[1] = g^[1](Z^[1])
            ...
            A^[m] = g^[m](Z^[m])
        
        Compute cost J^{t} with Regularization
        BackProp to optimize : 
            w^[l] := w^[l] - \alpha * dw^[l]
            b^[l] := b^[l] - \alpha * db^[l]
    }
}
```

### 2. 关于mini-batch的一些解释
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6iyy9z1kj30n407xq67.jpg)

* 在使用普通的batchtidu梯度下降算法时,每次迭代都会遍历整个训练集,因此可以预测每次代价函数的值都会随着迭代次数下降.
* 使用mini-batch梯度下降算法时,**代价函数并不是每次迭代都会下降**.因为在每次迭代中,要处理的是$X^{\{t\}},Y^{\{t\}}$,也就是说每次迭代下都在训练不同的mini-batch子样本集,因此会有很多噪声,但整体的平均走势应该是向下的.
* 需要决定的变量之一是mini-batch的大小.
    * 如果mini-batch太大,每次迭代需要处理大量的训练样本,单词迭代耗时会很长
    * 如果mini-batch过于小,梯度下降的随机性就很强,有时候会发生方向错误,这样会失去向量化带来的计算效率的提升.
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6jorxxiyj30cg08ktb1.jpg)
* 如果训练集较小(m<2000),直接使用batch梯度下降法就可以快速处理整个训练集.如果样本比较大,**一般选择mini-batch大小为64-512**.最好选择$2^n$,这样会运行的更快一些.


## 二.指数加权平均数
* $v_t = \beta v_{t-1} + (1-\beta)\theta_t$
    * $v_t$是第t个加权平均值
    * $\beta$是权重
    * $\theta_t$是数据集中第t个值


### 1. 指数加权平均示例
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6o2qhy1vj30f508cju2.jpg)  
假设希望通过一些温度数据计算出趋势,也就是温度的局部平均值:  
* 首先使第一天的温度$v_0 = 0$,然后每天用`0.9`作为加权数在之前的数值基础上加上当日温度的权重倍(1-0.9 = 0.1),即:  
    ![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6o6t880qj308q06fgmz.jpg)

    以计算出的每个数值作为当日的加权平均值,得到每日的指数加权平均值:  
    ![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6o8cnoyuj30eg08aq5j.jpg)

**公式即为: $v_t = \beta v_{t-1} + (1-\beta)\theta_t$**

**如果将权重设置为一个比较接近1的值**,比如0.98,这时得到的曲线就会相对平坦一些,并且会向右移动一些.因为在加权平均的过程中,当前值的权重更小,前面计算出的加权平均值的权重更大,(1-$\beta$比较小)变化幅度更小,所以会出现一定的延迟.

![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6on3m8rmj30f508dmzy.jpg)


### 2. 理解指数加权平均数
#### 计算加权平均值用到的数值的个数
假设$\beta = 0.9$,把括号展开:  
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6pxsoyjfj30lk015glh.jpg)  
可以看到,这是一个加权求和的过程,每一项数值都参与了求和,将每项数值与指数衰减函数相乘,第100天的加权平均值**包括从第1天到100天的数据**.  

* 由于$(0.9)^{10} \approx 0.35 \approx \frac 1 e$,即当天向前计数10天时,权重值相当于初始权重$\beta$的1/3,此时后面几项的权重会越来越小,可以忽略不计.这时可以认为,**计算这个指数加权平均数,只关注了过去10天的温度数据**.  
* 同样,在$\beta = 0.98$时,0.98^50大约等于1/e,因此初始权重为0.98可以看做平均了50天的温度数据.

**总结公式为:** 平均数值的个数 $\approx \frac 1 {1-\beta}$  
* $\beta$最常用的值为0.9

#### 利用加权平均数的优势
* 占用极少内存.由于数据是依次迭代计算出的,因此只要定义一个变量,此后不断用最新数据覆盖就可以了.虽然相对于计算平均数公式获得的平均数不是最精准的,但是不用保存所有的数据,计算成本低.

### 3.指数加权的偏差修正
由于第一天的初始平均值被初始化为0,因此后面得到的值会小很多  
为了修正这个问题,可以在**估测初期**不使用$v_t$,而是用$\frac {v_t}{1-\beta^t}$
* t是现在的天数
* 比如t=2时,$v2 = \frac {v_1\beta +(1-\beta)\theta_t}{1-\beta^t}$,也就是在原来的基础上除以偏差.
* 随着t的增加,$\beta^t$趋向于0.所以当t很大时,偏差修正几乎没有作用.所以在初期阶段偏差修正更有效

### 4.动量梯度下降法(Gradient descent with Momentum)
* 基本思想: 计算梯度的指数加权平均数,并利用该梯度更新权重  

![](http://ww1.sinaimg.cn/large/0077h8xtly1fszbdb04ggj30tr084af6.jpg)

我们希望在纵轴上学习慢一点,在横轴上加快学习.  
可以在每次迭代中,计算出当前mini-batch下的 dW,db后对它们做一个 **指数加权平均**,也就是
* $v_{dw} = \beta v_{dw} + (1-\beta)dw$
* $v_{db} = \beta v_{db} + (1-\beta)db$
* $W := W - av_{dw}$
* $b := b - a v_{db}$

经过这样的处理,在纵轴方向平均过程中,正负数相互抵消,所以平均值接近于0.但在横轴方向的平均与仍然比较大.这样就在抵达最小值的路上减少了摆动.

### 5.RMSprop
作用同样是在横向加快梯度下降.在使用动量梯度下降时,虽然横轴方向正在推进,但纵轴方向有大幅度的摆动.  所以想减缓纵向学习,加快横向学习.(这里把纵轴记为b,横轴记为w.也就是加快w的更新,减缓b的更新 )  
![](http://ww1.sinaimg.cn/large/0077h8xtly1ft1cwy4s7sj30io09k0xp.jpg)  
* 首先计算出当前mini-batch下的微分dW,db
* $S_{dw} = \beta S_{dw} + (1-\beta) dW^2$
* $S_{db} = \beta S_{db} + (1-\beta)db^2$
* $W:= W - \alpha \frac{dW}{\sqrt{S_{dw} + \epsilon}}$
* $b:= b - \alpha \frac{db}{\sqrt{S_{db} + \epsilon}}$
    * 分母加$\epsilon$的作用是为了避免分母为0的出现.通常选一个很小的数.$10^{-8}$就能够保证数值稳定一些.

这样处理后可以用一个更大的学习率alpha加快学习.

 ### 6.Adam优化算法--结合Momentum和RMSprop
 * 首先初始化$v_{dw} = 0,v_{db} = 0, S_{dw} = 0, S_{db} = 0$
 * 在第t次迭代中:
    * 用当前mini-batch计算出dW,db
    * $v_{dw} = \beta_1 v_{dw} + (1-\beta_1)dw$,$v_{db} = \beta_1 v_{db} + (1-\beta_1)db$
    * $S_{dw} = \beta_2 S_{dw} + (1-\beta_2) dw^2$, $S_{db} = \beta_2 S_{db} + (1-\beta_2)db^2$
    * 修正值 $v_{dw}^{corrected} = v_{dw} / (1-\beta_1^t) , v_{db}^{corrected} = v_{db} / (1- \beta_1^t)$
    * 修正值 $ S_{dw} ^ {corrected} = S_{dw} / (1 - \beta_2^t),S_{db}^{corrected} = S_{db} / (1 - \beta_2^t)$
    * ![](http://ww1.sinaimg.cn/large/0077h8xtly1ft1lsqu1msj304w020745.jpg)
    * ![](http://ww1.sinaimg.cn/large/0077h8xtly1ft1lsdm88zj305h01xt8k.jpg)

**常用超参数值**: 
* $\beta_1 = 0.9$
* $\beta_2 = 0.999$
* $\epsilon = 10 ^{-8}$ 



### 7. 学习率衰减

* $\alpha = \frac{1}{1 + decay-rate * epochnum}\alpha_0$
*   $\alpha = 0.95^{epochnum}\alpha_0 $
* $\alpha = \frac{k}{\sqrt{epochnum}} \alpha_0$
* 离散下降  ![](http://ww1.sinaimg.cn/large/0077h8xtly1ftqn2s3b0zj30f808rdgs.jpg)

###  8. 局部最优问题

许多人认为梯度下降很容易陷入局部最优值.然而在**高维度空间**中,局部最优值的要求为 **在每个方向上都需要梯度为0**,如果有2w个方向,概率就是P = 2^-20000^,这样的概率非常小.  

更有可能遇到的是**鞍点**,这样在每个方向有可能是凸函数,也有可能是凹函数  

![](http://ww1.sinaimg.cn/large/0077h8xtly1ftqnnoa3euj30j90cc78v.jpg)



尽管局部最优不是问题,鞍点也存在问题: **平稳段会减缓学习**.平稳段是一块区域,其中的导数长时间接近于0.这也是Momentum等优化算法能够加速学习的地方