# 优化算法

## 一.Mini-batch梯度下降

### 1. Mini-batch梯度下降算法
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6h8njhupj30m108r444.jpg)
* 把一个很大的数据集切分为很多相同大小的子数据集,把这些子数据集组成矩阵然后计算可以提高计算效率.
* 用大括号来表示第几个mini子数据集

在进行梯度下降算法时,每次同时处理单个mini子数据集$X^{\{t\}},Y^{\{t\}}$

```
# 最外层循环针对epoch反复训练,即遍历了整个mini子数据集
repeat{
    # 假设把数据集分成5000个mini子数据集
    for t in 5000{      
        Forword prop on X^{t}:  # 前向传播计算出网络输出值
            Z^[1] = w^[1]X^{t} + b^[1]
            A^[1] = g^[1](Z^[1])
            ...
            A^[m] = g^[m](Z^[m])
        
        Compute cost J^{t} with Regularization
        BackProp to optimize : 
            w^[l] := w^[l] - \alpha * dw^[l]
            b^[l] := b^[l] - \alpha * db^[l]
    }
}
```

### 2. 关于mini-batch的一些解释
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6iyy9z1kj30n407xq67.jpg)

* 在使用普通的batchtidu梯度下降算法时,每次迭代都会遍历整个训练集,因此可以预测每次代价函数的值都会随着迭代次数下降.
* 使用mini-batch梯度下降算法时,**代价函数并不是每次迭代都会下降**.因为在每次迭代中,要处理的是$X^{\{t\}},Y^{\{t\}}$,也就是说每次迭代下都在训练不同的mini-batch子样本集,因此会有很多噪声,但整体的平均走势应该是向下的.
* 需要决定的变量之一是mini-batch的大小.
    * 如果mini-batch太大,每次迭代需要处理大量的训练样本,单词迭代耗时会很长
    * 如果mini-batch过于小,梯度下降的随机性就很强,有时候会发生方向错误,这样会失去向量化带来的计算效率的提升.
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6jorxxiyj30cg08ktb1.jpg)
* 如果训练集较小(m<2000),直接使用batch梯度下降法就可以快速处理整个训练集.如果样本比较大,**一般选择mini-batch大小为64-512**.最好选择$2^n$,这样会运行的更快一些.


## 二.指数加权平均数
* $v_t = \beta v_{t-1} + (1-\beta)\theta_t$
    * $v_t$是第t个加权平均值
    * $\beta$是权重
    * $\theta_t$是数据集中第t个值


### 1. 指数加权平均示例
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6o2qhy1vj30f508cju2.jpg)  
假设希望通过一些温度数据计算出趋势,也就是温度的局部平均值:  
* 首先使第一天的温度$v_0 = 0$,然后每天用`0.9`作为加权数在之前的数值基础上加上当日温度的权重倍(1-0.9 = 0.1),即:  
    ![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6o6t880qj308q06fgmz.jpg)

    以计算出的每个数值作为当日的加权平均值,得到每日的指数加权平均值:  
    ![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6o8cnoyuj30eg08aq5j.jpg)

**公式即为: $v_t = \beta v_{t-1} + (1-\beta)\theta_t$**

如果将权重设置为一个比较接近1的值,比如0.98,这时得到的曲线就会相对平坦一些,并且会向右移动一些.因为在加权平均的过程中,(1-$\beta$比较小)变化幅度更小,所以会出现一定的延迟.

![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6on3m8rmj30f508dmzy.jpg)


### 2. 理解指数加权平均数
#### 计算加权平均值用到的数值的个数
假设$\beta = 0.9$,把括号展开:  
![](https://ws1.sinaimg.cn/large/0077h8xtly1fq6pxsoyjfj30lk015glh.jpg)  
可以看到,这是一个加权求和的过程,第100天的加权平均值**包括从第1天到100天的数据**.  

* 由于$(0.9)^{10} \approx 0.35 \approx \frac 1 e$,即当天向前计数10天时,权重值相当于初始权重$\beta$的1/3,此时后面几项的权重会越来越小,可以忽略不计.这时可以认为,**计算这个指数加权平均数,只关注了过去10天的温度数据**.  
* 同样,在$\beta = 0.98$时,0.98^50大约等于1/e,因此初始权重为0.98可以看做平均了50天的温度数据.

**总结公式为:** 平均数值的个数 $\approx \frac 1 {1-\beta}$  

#### 利用加权平均数的优势
* 占用极少内存.由于数据是依次迭代计算出的,因此只要定义一个变量,此后不断用最新数据覆盖就可以了.虽然相对于计算平均数公式获得的平均数不是最精准的,但是不用保存所有的数据,计算成本低.

### 3.指数加权的偏差修正
由于第一天的初始平均值被初始化为0,因此后面得到的值会小很多  
为了修正这个问题,可以在**估测初期**不使用$v_t$,而是用$\frac {v_t}{1-\beta^t}$
* t是现在的天数
* 比如t=2时,$v2 = \frac {v_1\beta +(1-\beta)\theta_t}{1-\beta^t}$,也就是在原来的基础上除以偏差.
* 随着t的增加,$\beta^t$趋向于0.所以当t很大时,偏差修正几乎没有作用.所以在初期阶段偏差修正更有效

## 三.动量梯度下降法(Gradient descent with Momentum)


